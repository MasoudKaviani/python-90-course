{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f9622945156d6337ba73c481da2de7efef7384"
   },
   "source": [
    "## <div style=\"text-align: center\">A Comprehensive Machine Learning Workflow with Python </div>\n",
    "\n",
    "<div style=\"text-align: center\">There are plenty of <b>courses and tutorials</b> that can help you learn machine learning from scratch but here in <b>Kaggle</b>, I want to predict <b>House prices</b>(in the next version)  a popular machine learning Dataset as a comprehensive workflow with python packages. \n",
    "After reading, you can use this workflow to solve other real problems and use it as a template to deal with <b>machine learning</b> problems.</div>\n",
    "<div style=\"text-align:center\">last update: <b>10/16/2018</b></div>\n",
    "\n",
    "\n",
    "\n",
    ">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https://github.com/mjbahmani/10-steps-to-become-a-data-scientist)\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------\n",
    "you can Fork and Run this kernel on Github:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani/Machine-Learning-Workflow-with-Python)\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    " **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cda11210a88d6484112cbe2c3624225328326c6a"
   },
   "source": [
    "## Notebook  Content\n",
    "*   1-  [Introduction](#1)\n",
    "*   2- [Machine learning workflow](#2)\n",
    "*       2-1 [Real world Application Vs Competitions](#2)\n",
    "\n",
    "*   3- [Problem Definition](#3)\n",
    "*       3-1 [Problem feature](#4)\n",
    "*       3-2 [Aim](#5)\n",
    "*       3-3 [Variables](#6)\n",
    "*   4-[ Inputs & Outputs](#7)\n",
    "*   4-1 [Inputs ](#8)\n",
    "*   4-2 [Outputs](#9)\n",
    "*   5- [Installation](#10)\n",
    "*       5-1 [ jupyter notebook](#11)\n",
    "*       5-2[ kaggle kernel](#12)\n",
    "*       5-3 [Colab notebook](#13)\n",
    "*       5-4 [install python & packages](#14)\n",
    "*       5-5 [Loading Packages](#15)\n",
    "*   6- [Exploratory data analysis](#16)\n",
    "*       6-1 [Data Collection](#17)\n",
    "*       6-2 [Visualization](#18)\n",
    "*           6-2-1 [Scatter plot](#19)\n",
    "*           6-2-2 [Box](#20)\n",
    "*           6-2-3 [Histogram](#21)\n",
    "*           6-2-4 [Multivariate Plots](#22)\n",
    "*           6-2-5 [Violinplots](#23)\n",
    "*           6-2-6 [Pair plot](#24)\n",
    "*           6-2-7 [Kde plot](#25)\n",
    "*           6-2-8 [Joint plot](#26)\n",
    "*           6-2-9 [Andrews curves](#27)\n",
    "*           6-2-10 [Heatmap](#28)\n",
    "*           6-2-11 [Radviz](#29)\n",
    "*       6-3 [Data Preprocessing](#30)\n",
    "*       6-4 [Data Cleaning](#31)\n",
    "*   7- [Model Deployment](#32)\n",
    "*       7-1[ KNN](#33)\n",
    "*       7-2 [Radius Neighbors Classifier](#34)\n",
    "*       7-3 [Logistic Regression](#35)\n",
    "*       7-4 [Passive Aggressive Classifier](#36)\n",
    "*       7-5 [Naive Bayes](#37)\n",
    "*       7-6 [MultinomialNB](#38)\n",
    "*       7-7 [BernoulliNB](#39)\n",
    "*       7-8 [SVM](#40)\n",
    "*       7-9 [Nu-Support Vector Classification](#41)\n",
    "*       7-10 [Linear Support Vector Classification](#42)\n",
    "*       7-11 [Decision Tree](#43)\n",
    "*       7-12 [ExtraTreeClassifier](#44)\n",
    "*       7-13 [Neural network](#45)\n",
    "*            7-13-1 [What is a Perceptron?](#45)\n",
    "*       7-14 [RandomForest](#46)\n",
    "*       7-15 [Bagging classifier ](#47)\n",
    "*       7-16 [AdaBoost classifier](#48)\n",
    "*       7-17 [Gradient Boosting Classifier](#49)\n",
    "*       7-18 [Linear Discriminant Analysis](#50)\n",
    "*       7-19 [Quadratic Discriminant Analysis](#51)\n",
    "*       7-20 [Kmeans](#52)\n",
    "*       7-21 [Backpropagation](#53)\n",
    "*   9- [Conclusion](#54)\n",
    "*  10- [References](#55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "750903cc2679d39058f56df6c6c040be02b748df"
   },
   "source": [
    " <a id=\"1\"></a> <br>\n",
    "## 1- Introduction\n",
    "This is a **comprehensive ML techniques with python** , that I have spent for more than two months to complete it.\n",
    "\n",
    "it is clear that everyone in this community is familiar with IRIS dataset but if you need to review your information about the dataset please visit this [link](https://archive.ics.uci.edu/ml/datasets/iris).\n",
    "\n",
    "I have tried to help **beginners**  in Kaggle how to face machine learning problems. and I think it is a great opportunity for who want to learn machine learning workflow with python completely.\n",
    "I have covered most of the methods that are implemented for iris until **2018**, you can start to learn and review your knowledge about ML with a simple dataset and try to learn and memorize the workflow for your journey in Data science world.\n",
    "\n",
    "I am open to getting your feedback for improving this **kernel**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e11b73b618b0f6e4335520ef80267c6d577d1ba5"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2- Machine Learning Workflow\n",
    "Field of \tstudy \tthat \tgives\tcomputers\tthe\tability \tto\tlearn \twithout \tbeing\n",
    "explicitly \tprogrammed.\n",
    "\n",
    "Arthur\tSamuel, 1959\n",
    "\n",
    "If you have already read some [machine learning books](https://towardsdatascience.com/list-of-free-must-read-machine-learning-books-89576749d2ff). You have noticed that there are different ways to stream data into machine learning.\n",
    "\n",
    "most of these books share the following steps (checklist):\n",
    "*   Define the Problem(Look at the big picture)\n",
    "*   Specify Inputs & Outputs\n",
    "*   Data Collection\n",
    "*   Exploratory data analysis\n",
    "*   Data Preprocessing\n",
    "*   Model Design, Training, and Offline Evaluation\n",
    "*   Model Deployment, Online Evaluation, and Monitoring\n",
    "*   Model Maintenance, Diagnosis, and Retraining\n",
    "\n",
    "**You can see my workflow in the below image** :\n",
    " <img src=\"http://s9.picofile.com/file/8338227634/workflow.png\" />\n",
    "\n",
    "**you should\tfeel free\tto\tadapt \tthis\tchecklist \tto\tyour needs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8b27260d5ccff025f37490d84bd35bba7eef00a"
   },
   "source": [
    "## 2-1 Real world Application Vs Competitions\n",
    "<img src=\"http://s9.picofile.com/file/8339956300/reallife.png\" height=\"600\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "600be852c0d28e7c0c5ebb718904ab15a536342c"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3- Problem Definition\n",
    "I think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n",
    "\n",
    "Problem Definition has four steps that have illustrated in the picture below:\n",
    "<img src=\"http://s8.picofile.com/file/8338227734/ProblemDefination.png\">\n",
    "<a id=\"4\"></a> <br>\n",
    "### 3-1 Problem Feature\n",
    "we will use the classic Iris data set. This dataset contains information about three different types of Iris flowers:\n",
    "\n",
    "* Iris Versicolor\n",
    "* Iris Virginica\n",
    "* Iris Setosa\n",
    "\n",
    "The data set contains measurements of four variables :\n",
    "\n",
    "* sepal length \n",
    "* sepal width\n",
    "* petal length \n",
    "* petal width\n",
    " \n",
    "The Iris data set has a number of interesting features:\n",
    "\n",
    "1. One of the classes (Iris Setosa) is linearly separable from the other two. However, the other two classes are not linearly separable.\n",
    "\n",
    "2. There is some overlap between the Versicolor and Virginica classes, so it is unlikely to achieve a perfect classification rate.\n",
    "\n",
    "3. There is some redundancy in the four input variables, so it is possible to achieve a good solution with only three of them, or even (with difficulty) from two, but the precise choice of best variables is not obvious.\n",
    "\n",
    "**Why am I  using iris dataset:**\n",
    "\n",
    "1- This is a good project because it is so well understood.\n",
    "\n",
    "2- Attributes are numeric so you have to figure out how to load and handle data.\n",
    "\n",
    "3- It is a classification problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n",
    "\n",
    "4- It is a multi-class classification problem (multi-nominal) that may require some specialized handling.\n",
    "\n",
    "5- It only has 4 attributes and 150 rows, meaning it is small and easily fits into memory (and a screen or A4 page).\n",
    "\n",
    "6- All of the numeric attributes are in the same units and the same scale, not requiring any special scaling or transforms to get started.[5]\n",
    "\n",
    "7- we can define problem as clustering(unsupervised algorithm) project too.\n",
    "<a id=\"5\"></a> <br>\n",
    "### 3-2 Aim\n",
    "The aim is to classify iris flowers among three species (setosa, versicolor or virginica) from measurements of length and width of sepals and petals\n",
    "<a id=\"6\"></a> <br>\n",
    "### 3-3 Variables\n",
    "The variables are :\n",
    "**sepal_length**: Sepal length, in centimeters, used as input.\n",
    "**sepal_width**: Sepal width, in centimeters, used as input.\n",
    "**petal_length**: Petal length, in centimeters, used as input.\n",
    "**petal_width**: Petal width, in centimeters, used as input.\n",
    "**setosa**: Iris setosa, true or false, used as target.\n",
    "**versicolour**: Iris versicolour, true or false, used as target.\n",
    "**virginica**: Iris virginica, true or false, used as target.\n",
    "\n",
    "**<< Note >>**\n",
    "> You must answer the following question:\n",
    "How does your company expact to use and benfit from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bb4dfebb521f83543e1d45db3559216dad8f6fb"
   },
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## 4- Inputs & Outputs\n",
    "<a id=\"8\"></a> <br>\n",
    "### 4-1 Inputs\n",
    "**Iris** is a very popular **classification** and **clustering** problem in machine learning and it is such as \"Hello world\" program when you start learning a new programming language. then I decided to apply Iris on  20 machine learning method on it.\n",
    "The Iris flower data set or Fisher's Iris data set is a **multivariate data set** introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers in three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "As a result, **iris dataset is used as the input of all algorithms**.\n",
    "![iris](https://image.ibb.co/gbH3ue/iris.png)\n",
    "[image source](https://rpubs.com/wjholst/322258)\n",
    "<a id=\"9\"></a> <br>\n",
    "### 4-2 Outputs\n",
    "the outputs for our algorithms totally depend on the type of classification or clustering algorithms.\n",
    "the outputs can be the number of clusters or predict for new input.\n",
    "\n",
    "**setosa**: Iris setosa, true or false, used as target.\n",
    "**versicolour**: Iris versicolour, true or false, used as target.\n",
    "**virginica**: Iris virginica, true or false, used as a target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89ee0cda57822cd4102eadf8992c5bfe1964d557"
   },
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "## 5-Installation\n",
    "#### Windows:\n",
    "* Anaconda (from https://www.continuum.io) is a free Python distribution for SciPy stack. It is also available for Linux and Mac.\n",
    "* Canopy (https://www.enthought.com/products/canopy/) is available as free as well as commercial distribution with full SciPy stack for Windows, Linux and Mac.\n",
    "* Python (x,y) is a free Python distribution with SciPy stack and Spyder IDE for Windows OS. (Downloadable from http://python-xy.github.io/)\n",
    "#### Linux\n",
    "Package managers of respective Linux distributions are used to install one or more packages in SciPy stack.\n",
    "\n",
    "For Ubuntu Users:\n",
    "sudo apt-get install python-numpy python-scipy python-matplotlibipythonipythonnotebook\n",
    "python-pandas python-sympy python-nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c1793fb141d3338bbc4300874be6ffa5cb1a9139"
   },
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "## 5-1 Jupyter notebook\n",
    "I strongly recommend installing **Python** and **Jupyter** using the **[Anaconda Distribution](https://www.anaconda.com/download/)**, which includes Python, the Jupyter Notebook, and other commonly used packages for scientific computing and data science.\n",
    "\n",
    "First, download Anaconda. We recommend downloading Anaconda’s latest Python 3 version.\n",
    "\n",
    "Second, install the version of Anaconda which you downloaded, following the instructions on the download page.\n",
    "\n",
    "Congratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abbd1757dde9805758a2cec47a186e31dbc29822"
   },
   "source": [
    "> jupyter notebook\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a70c253d5afa93f07a7a7e048dbb2d7812c8d10"
   },
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "## 5-2 Kaggle Kernel\n",
    "Kaggle kernel is an environment just like you use jupyter notebook, it's an **extension** of the where in you are able to carry out all the functions of jupyter notebooks plus it has some added tools like forking et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "237bbe4e4509c9491ce165e3599c432b979d7b90"
   },
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "## 5-3 Colab notebook\n",
    "**Colaboratory** is a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use.\n",
    "### 5-3-1 What browsers are supported?\n",
    "Colaboratory works with most major browsers, and is most thoroughly tested with desktop versions of Chrome and Firefox.\n",
    "### 5-3-2 Is it free to use?\n",
    "Yes. Colaboratory is a research project that is free to use.\n",
    "### 5-3-3 What is the difference between Jupyter and Colaboratory?\n",
    "Jupyter is the open source project on which Colaboratory is based. Colaboratory allows you to use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer other than a browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbedcae8843986c2139f18dad4b5f313e6535ac5"
   },
   "source": [
    "<a id=\"15\"></a> <br>\n",
    "## 5-5 Loading Packages\n",
    "In this kernel we are using the following packages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "61f49281fdd8592b44c0867225f57e6fce36342c"
   },
   "source": [
    " <img src=\"http://s8.picofile.com/file/8338227868/packages.png\">\n",
    " Now we import all of them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages to load \n",
    "# Check the versions of libraries\n",
    "# Python version\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "print('Python: {}'.format(sys.version))\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "import numpy\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "# numpy\n",
    "import numpy as np # linear algebra\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "# pandas\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "import tensorflow as tf\n",
    "print('tensorflow: {}'.format(tf.__version__))\n",
    "%matplotlib inline\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Importing metrics for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from pandas import get_dummies\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04ff1a533119d589baee777c21194a951168b0c7"
   },
   "source": [
    "<a id=\"16\"></a> <br>\n",
    "## 6- Exploratory Data Analysis(EDA)\n",
    " In this section, you'll learn how to use graphical and numerical techniques to begin uncovering the structure of your data. \n",
    " \n",
    "* Which variables suggest interesting relationships?\n",
    "* Which observations are unusual?\n",
    "\n",
    "By the end of the section, you'll be able to answer these questions and more, while generating graphics that are both insightful and beautiful.  then We will review analytical and statistical operations:\n",
    "\n",
    "*   5-1 Data Collection\n",
    "*   5-2 Visualization\n",
    "*   5-3 Data Preprocessing\n",
    "*   5-4 Data Cleaning\n",
    "<img src=\"http://s9.picofile.com/file/8338476134/EDA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cedecea930b278f86292367cc28d2996a235a169"
   },
   "source": [
    "<a id=\"17\"></a> <br>\n",
    "## 6-1 Data Collection\n",
    "**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n",
    "\n",
    "**Iris dataset**  consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9269ae851b744856bce56840637030a16a5877e1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Dataset to play with it\n",
    "dataset = pd.read_csv('../input/Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58ed9c838069f54de5cf90b20a774c3e236149b3"
   },
   "source": [
    "**<< Note 1 >>**\n",
    "\n",
    "* Each row is an observation (also known as : sample, example, instance, record)\n",
    "* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b5fd1034cd591ebd29fba1c77d342ec2b408d13"
   },
   "source": [
    "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edd043f8feb76cfe51b79785302ca4936ceb7b51",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "055772bd170aa8018aabd85106b76675802c33b3"
   },
   "source": [
    "<a id=\"18\"></a> <br>\n",
    "## 6-2 Visualization\n",
    "**Data visualization**  is the presentation of data in a pictorial or graphical format. It enables decision makers to see analytics presented visually, so they can grasp difficult concepts or identify new patterns.\n",
    "\n",
    "With interactive visualization, you can take the concept a step further by using technology to drill down into charts and graphs for more detail, interactively changing what data you see and how it’s processed.[SAS]\n",
    "\n",
    " In this section I show you  **11 plots** with **matplotlib** and **seaborn** that is listed in the blew picture:\n",
    " <img src=\"http://s8.picofile.com/file/8338475500/visualization.jpg\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b0014a7a52e714996bc443981c853095926d20e5"
   },
   "source": [
    "<a id=\"19\"></a> <br>\n",
    "### 6-2-1 Scatter plot\n",
    "\n",
    "Scatter plot Purpose To identify the type of relationship (if any) between two quantitative variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af099546eed64ebc796403d4139cb4c977c27b03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modify the graph above by assigning each species an individual color.\n",
    "sns.FacetGrid(dataset, hue=\"Species\", size=5) \\\n",
    "   .map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\") \\\n",
    "   .add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1c7b62b5f8cba427bca13049256365141655372"
   },
   "source": [
    "<a id=\"20\"></a> <br>\n",
    "### 6-2-2 Box\n",
    "In descriptive statistics, a **box plot** or boxplot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram.[wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0655e20f31a582f861d391308a088778cd7eaae9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.plot(kind='box', subplots=True, layout=(2,3), sharex=False, sharey=False)\n",
    "plt.figure()\n",
    "#This gives us a much clearer idea of the distribution of the input attributes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7f6426fd44bcd77d35a5fdbc8c4fc4f18d991ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot the species data using a box plot:\n",
    "\n",
    "sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b193e4aa7e6fb337d3f65c334849094addd097a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Seaborn's striplot to add data points on top of the box plot \n",
    "# Insert jitter=True so that the data points remain scattered and not piled into a verticle line.\n",
    "# Assign ax to each axis, so that each plot is ontop of the previous axis. \n",
    "\n",
    "ax= sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset)\n",
    "ax= sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset, jitter=True, edgecolor=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56adbafa89c117118621c72b3b7cb19edc21298e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweek the plot above to change fill and border color color using ax.artists.\n",
    "# Assing ax.artists a variable name, and insert the box number into the corresponding brackets\n",
    "\n",
    "ax= sns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset)\n",
    "ax= sns.stripplot(x=\"Species\", y=\"PetalLengthCm\", data=dataset, jitter=True, edgecolor=\"gray\")\n",
    "\n",
    "boxtwo = ax.artists[2]\n",
    "boxtwo.set_facecolor('red')\n",
    "boxtwo.set_edgecolor('black')\n",
    "boxthree=ax.artists[1]\n",
    "boxthree.set_facecolor('yellow')\n",
    "boxthree.set_edgecolor('black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "743a92c3c2fff1a1f99845518247f7971ad18b7c"
   },
   "source": [
    "<a id=\"21\"></a> <br>\n",
    "### 6-2-3 Histogram\n",
    "We can also create a **histogram** of each input variable to get an idea of the distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5da0520ed3e738ee8814b2d91843ed4acec2b6e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histograms\n",
    "dataset.hist(figsize=(15,20))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b4e3de19781686010c6038f0e3076eb678398169"
   },
   "source": [
    "It looks like perhaps two of the input variables have a Gaussian distribution. This is useful to note as we can use algorithms that can exploit this assumption.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f80a6e971cbf0af72d659b51af552ea1dddc9a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"PetalLengthCm\"].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bbff56707484f88625eb8ef309b712ba03f939e"
   },
   "source": [
    "<a id=\"22\"></a> <br>\n",
    "### 6-2-4 Multivariate Plots\n",
    "Now we can look at the interactions between the variables.\n",
    "\n",
    "First, let’s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb4e5d117e4ef40d7668632f42130206a5537bd0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# scatter plot matrix\n",
    "pd.plotting.scatter_matrix(dataset,figsize=(10,10))\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de7fea7986071fafbe0b93933e3beda445cbe373"
   },
   "source": [
    "Note the diagonal grouping of some pairs of attributes. This suggests a high correlation and a predictable relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0f696ec021ec99c1058a62e22c8b73082fe6fa7"
   },
   "source": [
    "<a id=\"23\"></a> <br>\n",
    "### 6-2-5 violinplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e352d2f8340609adf4bf6718b1d2ecee0fa730b5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# violinplots on petal-length for each species\n",
    "sns.violinplot(data=dataset,x=\"Species\", y=\"PetalLengthCm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ed35bceb87051e56316d35a630334518e8b8c64"
   },
   "source": [
    "<a id=\"24\"></a> <br>\n",
    "### 6-2-6 pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b80350add6f9a742f10bffc4b497562f8bebea95",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using seaborn pairplot to see the bivariate relation between each pair of features\n",
    "sns.pairplot(dataset, hue=\"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb187bcc0fb51e53f8abe9e3952c6ae5c3177411"
   },
   "source": [
    "From the plot, we can see that the species setosa is separataed from the other two across all feature combinations\n",
    "\n",
    "We can also replace the histograms shown in the diagonal of the pairplot by kde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5570ff32db5a4740b26b244531af552ac1b57f4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# updating the diagonal elements in a pairplot to show a kde\n",
    "sns.pairplot(dataset, hue=\"Species\",diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2544d3c2dd34a360d295019d8cb597c7ef8f66bc"
   },
   "source": [
    "<a id=\"25\"></a> <br>\n",
    "###  6-2-7 kdeplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d07222b89303b386e9e824d52cc73c045667f25",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seaborn's kdeplot, plots univariate or bivariate density estimates.\n",
    "#Size can be changed by tweeking the value used\n",
    "sns.FacetGrid(dataset, hue=\"Species\", size=5).map(sns.kdeplot, \"PetalLengthCm\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "560d8e8f17bacefaf8c3855a9648f26b82fdee9b"
   },
   "source": [
    "<a id=\"26\"></a> <br>\n",
    "### 6-2-8 jointplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4adb4da16ea61e0f1a12bc9925dfbbaaa81e0360",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use seaborn's jointplot to make a hexagonal bin plot\n",
    "#Set desired size and ratio and choose a color.\n",
    "sns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=dataset, size=10,ratio=10, kind='hex',color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3768e31e990bfe4c2ff7b45087fbba85e0560d00"
   },
   "source": [
    "<a id=\"27\"></a> <br>\n",
    "###  6-2-9 andrews_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "937b6856d109001db14a3ac99568df45efbe1070",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In Pandas use Andrews Curves to plot and visualize data structure.\n",
    "#Each multivariate observation is transformed into a curve and represents the coefficients of a Fourier series.\n",
    "#This useful for detecting outliers in times series data.\n",
    "#Use colormap to change the color of the curves\n",
    "\n",
    "from pandas.tools.plotting import andrews_curves\n",
    "andrews_curves(dataset.drop(\"Id\", axis=1), \"Species\",colormap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "263eaa9d2bfad0f8c68b6e8e874bdc11a6e802ac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use seaborn jointplot shows bivariate scatterplots and univariate histograms with Kernel density \n",
    "# estimation in the same figure\n",
    "sns.jointplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=dataset, size=6, kind='kde', color='#800000', space=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e73333289d17dd648b7b2112d7fe3fe7ea444d0"
   },
   "source": [
    "<a id=\"28\"></a> <br>\n",
    "### 6-2-10 Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3100955ca9dc61ac7d435e9c064d10d06f26afa7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4)) \n",
    "sns.heatmap(dataset.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b81dbdd5dd8cb92a86b1f7114ffb6f088458a527"
   },
   "source": [
    "<a id=\"29\"></a> <br>\n",
    "### 6-2-11 radviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33fed3027d7242227d612a84bbb42b012356091b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A final multivariate visualization technique pandas has is radviz\n",
    "# Which puts each feature as a point on a 2D plane, and then simulates\n",
    "# having each sample attached to those points through a spring weighted\n",
    "# by the relative value for that feature\n",
    "from pandas.tools.plotting import radviz\n",
    "radviz(dataset.drop(\"Id\", axis=1), \"Species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab06d1cd799430c7c7f8de978ee2c6e275e7655b"
   },
   "source": [
    "###  Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "329488de1a908a6d367b9da4b40a20238163d32e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['Species'].value_counts().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0859caf857ceeb19f4cc47ccd11fbbfdfe4b0dd9"
   },
   "source": [
    "**<< Note >>**\n",
    "\n",
    "**Yellowbrick** is a suite of visual diagnostic tools called “Visualizers” that extend the Scikit-Learn API to allow human steering of the model selection process. In a nutshell, Yellowbrick combines scikit-learn with matplotlib in the best tradition of the scikit-learn documentation, but to produce visualizations for your models! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5af51158a5bc342947c553392e3d1665ac24ba62"
   },
   "source": [
    "### 6-2-12 Conclusion\n",
    "we have used Python to apply data visualization tools to the Iris dataset. Color and size changes were made to the data points in scatterplots. I changed the border and fill color of the boxplot and violin, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "91dda1f631cf4ed362162501aaaac6d19cfd6cc7"
   },
   "source": [
    "<a id=\"30\"></a> <br>\n",
    "## 6-3 Data Preprocessing\n",
    "**Data preprocessing** refers to the transformations applied to our data before feeding it to the algorithm.\n",
    " \n",
    "Data Preprocessing is a technique that is used to convert the raw data into a clean data set. In other words, whenever the data is gathered from different sources it is collected in raw format which is not feasible for the analysis.\n",
    "there are plenty of steps for data preprocessing and we just listed some of them :\n",
    "* removing Target column (id)\n",
    "* Sampling (without replacement)\n",
    "* Making part of iris unbalanced and balancing (with undersampling and SMOTE)\n",
    "* Introducing missing values and treating them (replacing by average values)\n",
    "* Noise filtering\n",
    "* Data discretization\n",
    "* Normalization and standardization\n",
    "* PCA analysis\n",
    "* Feature selection (filter, embedded, wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "581b90e6a869c3793472c7edd59091d6d6342fb2"
   },
   "source": [
    "## 6-3-1 Features\n",
    "Features:\n",
    "* numeric\n",
    "* categorical\n",
    "* ordinal\n",
    "* datetime\n",
    "* coordinates\n",
    "\n",
    "find the type of features in titanic dataset\n",
    "<img src=\"http://s9.picofile.com/file/8339959442/titanic.png\" height=\"700\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73ab30f86273b590a51fc363d9bf78c2709558fa"
   },
   "source": [
    "### 6-3-2 Explorer Dataset\n",
    "1- Dimensions of the dataset.\n",
    "\n",
    "2- Peek at the data itself.\n",
    "\n",
    "3- Statistical summary of all attributes.\n",
    "\n",
    "4- Breakdown of the data by the class variable.[7]\n",
    "\n",
    "Don’t worry, each look at the data is **one command**. These are useful commands that you can use again and again on future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b45251be7be77333051fe738639104ae1005fa5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shape\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c64e9d3e0bf394fb833de94a0fc5c34f69fce24c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#columns*rows\n",
    "dataset.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6315bf510cecb907b2d23aad25faf6ccad32ac4"
   },
   "source": [
    "how many NA elements in every column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "675f72fb58d83c527f71819e71ed8e17f81126f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8e124ca20643ad307d9bfdc34328d548c6ddcbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove rows that have NA's\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "277e1998627d6a3ddeff4e913a6b8c3dc81dec96"
   },
   "source": [
    "\n",
    "We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property.\n",
    "\n",
    "You should see 150 instances and 5 attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95ee5e18f97bc410df1e54ac74e32cdff2b30755"
   },
   "source": [
    "for getting some information about the dataset you can use **info()** command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca840f02925751186f87e402fcb5f637ab1ab8a0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3458838205be4c7fbff88e95ef69934e13e2199b"
   },
   "source": [
    "you see number of unique item for Species with command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b90d165a007106ae99809ad28edd75bd8153dd8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['Species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8accfbddf2228274ad412c3ad3be72b4107d6f6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"Species\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae08b544a8d4202c7d0a47ec83d685e81c91a66d"
   },
   "source": [
    "to check the first 5 rows of the data set, we can use head(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5899889553c3416b27e93efceddb106eb71f5156",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.head(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1150b6ac3d82562aefd5c64f9f01accee5eace4d"
   },
   "source": [
    "to check out last 5 row of the data set, we use tail() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79339442ff1f53ae1054d794337b9541295d3305",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c288c3dc8656a872a8529368812546e434d3a22"
   },
   "source": [
    "to pop up 5 random rows from the data set, we can use **sample(5)**  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09eb18d1fcf4a2b73ba2f5ddce99dfa521681140",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.sample(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8a1cc36348c68fb98d6cb28aa9919fc5f2892f3"
   },
   "source": [
    "to give a statistical summary about the dataset, we can use **describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f7211e96627b9a81c5b620a9ba61446f7719ea3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "031d16ca235837e889734635ecff193be64b27a4"
   },
   "source": [
    "to check out how many null info are on the dataset, we can use **isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8807b632269e2fa734ad26e8513199400fc09a83",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "446e6162e16325213047ff31454813455668b574",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.groupby('Species').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c2f1eaf0b6dfdc7cc4dace04614e99ed56425d00"
   },
   "source": [
    "to print dataset **columns**, we can use columns atribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "909d61b33ec06249d0842e6115597bbacf21163f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22bc5d81c18275ee1fb082c0adbb7a65bdbec4cc"
   },
   "source": [
    "**<< Note 2 >>**\n",
    "in pandas's data frame you can perform some query such as \"where\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8c8d9fd63d9bdb601183aeb4f1435affeb8a596",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.where(dataset ['Species']=='Iris-setosa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33fc33a18489b438a884819d99dc00a02b113be8"
   },
   "source": [
    "as you can see in the below in python, it is so easy perform some query on the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b545ff7e8367c5ab9c1db710f70b6936ac8422c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[dataset['SepalLengthCm']>7.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c92b300076a232321c915857d8a7c5685a97865",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperating the data into dependent and independent variables\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa882e5bcdc7d5f440489eff75d1d225269655a4"
   },
   "source": [
    "**<< Note >>**\n",
    ">**Preprocessing and generation pipelines depend on a model type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8280749a19af32869978c61941d1dea306632d71"
   },
   "source": [
    "<a id=\"31\"></a> <br>\n",
    "## 6-4 Data Cleaning\n",
    "When dealing with real-world data, dirty data is the norm rather than the exception. We continuously need to predict correct values, impute missing ones, and find links between various data artefacts such as schemas and records. We need to stop treating data cleaning as a piecemeal exercise (resolving different types of errors in isolation), and instead leverage all signals and resources (such as constraints, available statistics, and dictionaries) to accurately predict corrective actions.\n",
    "\n",
    "The primary goal of data cleaning is to detect and remove errors and **anomalies** to increase the value of data in analytics and decision making. While it has been the focus of many researchers for several years, individual problems have been addressed separately. These include missing value imputation, outliers detection, transformations, integrity constraints violations detection and repair, consistent query answering, deduplication, and many other related problems such as profiling and constraints mining.[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60dcf563b3a637f4836d5d3487b15a8f444caf53",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = dataset.columns\n",
    "features = cols[0:4]\n",
    "labels = cols[4]\n",
    "print(features)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "180b76d9afa9d51cbe05e38a128d7f1f63e6da26",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Well conditioned data will have zero mean and equal variance\n",
    "#We get this automattically when we calculate the Z Scores for the data\n",
    "\n",
    "data_norm = pd.DataFrame(dataset)\n",
    "\n",
    "for feature in features:\n",
    "    dataset[feature] = (dataset[feature] - dataset[feature].mean())/dataset[feature].std()\n",
    "\n",
    "#Show that should now have zero mean\n",
    "print(\"Averages\")\n",
    "print(dataset.mean())\n",
    "\n",
    "print(\"\\n Deviations\")\n",
    "#Show that we have equal variance\n",
    "print(pow(dataset.std(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c40ac6be7f6cff60f81e97478662817ef6ae1ef4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shuffle The data\n",
    "indices = data_norm.index.tolist()\n",
    "indices = np.array(indices)\n",
    "np.random.shuffle(indices)\n",
    "X = data_norm.reindex(indices)[features]\n",
    "y = data_norm.reindex(indices)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d260748e8e388fe03be57a8a120be513710a26c0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One Hot Encode as a dataframe\n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = get_dummies(y)\n",
    "\n",
    "# Generate Training and Validation Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.3)\n",
    "\n",
    "# Convert to np arrays so that we can use with TensorFlow\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test  = np.array(X_test).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32)\n",
    "y_test  = np.array(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94ff73f511d1aea70025681c4c04200bf2973dcc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check to make sure split still has 4 features and 3 labels\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cc7c7b60a33390a85b16bc34e3b9e424650cdd"
   },
   "source": [
    "<a id=\"32\"></a> <br>\n",
    "## 7- Model Deployment\n",
    "In this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n",
    "\n",
    "> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b7788bbaaace438242d3b2d0d2ed489a91939ce"
   },
   "source": [
    "## Families of ML algorithms\n",
    "There are several categories for machine learning algorithms, below are some of these categories:\n",
    "* Linear\n",
    "    * Linear Regression\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines\n",
    "* Tree-Based\n",
    "    * Decision Tree\n",
    "    * Random Forest\n",
    "    * GBDT\n",
    "* KNN\n",
    "* Neural Networks\n",
    "\n",
    "-----------------------------\n",
    "And if we  want to categorize ML algorithms with the type of learning, there are below type:\n",
    "* Classification\n",
    "\n",
    "    * k-Nearest \tNeighbors\n",
    "    * LinearRegression\n",
    "    * SVM\n",
    "    * DT \n",
    "    * NN\n",
    "    \n",
    "* clustering\n",
    "\n",
    "    * K-means\n",
    "    * HCA\n",
    "    * Expectation Maximization\n",
    "    \n",
    "* Visualization \tand\tdimensionality \treduction:\n",
    "\n",
    "    * Principal \tComponent \tAnalysis(PCA)\n",
    "    * Kernel PCA\n",
    "    * Locally -Linear\tEmbedding \t(LLE)\n",
    "    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n",
    "    \n",
    "* Association \trule\tlearning\n",
    "\n",
    "    * Apriori\n",
    "    * Eclat\n",
    "* Semisupervised learning\n",
    "* Reinforcement Learning\n",
    "    * Q-learning\n",
    "* Batch learning & Online learning\n",
    "* Ensemble  Learning\n",
    "\n",
    "**<< Note >>**\n",
    "> Here is no method which outperforms all others for all tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "daf9910caba26e071ff560dbdaca079ee148e140"
   },
   "source": [
    "<a id=\"33\"></a> <br>\n",
    "## Prepare Features & Targets\n",
    "First of all seperating the data into dependent(Feature) and independent(Target) variables.\n",
    "\n",
    "**<< Note 4 >>**\n",
    "* X==>>Feature\n",
    "* y==>>Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b06cb1191a0f52a904c52a918d1f999536e79bda",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d13f167dd92888d856c4ad2ff2895bf4855e361c"
   },
   "source": [
    "## Accuracy and precision\n",
    "* **precision** : \n",
    "\n",
    "In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n",
    "* **recall** : \n",
    "\n",
    "recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n",
    "* **F-score** :\n",
    "\n",
    "the F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "**What is the difference between accuracy and precision?\n",
    "\"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8b544762cc789bfeb8ebccd6765f77b9c7e1a0f"
   },
   "source": [
    "<a id=\"33\"></a> <br>\n",
    "## 7-1 K-Nearest Neighbours\n",
    "In **Machine Learning**, the **k-nearest neighbors algorithm** (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n",
    "\n",
    "In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eaa2caacfbc319932f79c75c549364089d1e649f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# K-Nearest Neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "Model = KNeighborsClassifier(n_neighbors=8)\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e01bbec9f80532e30c6cf26d5c3fffffb5ea01d4"
   },
   "source": [
    "<a id=\"34\"></a> <br>\n",
    "##  7-2 Radius Neighbors Classifier\n",
    "Classifier implementing a **vote** among neighbors within a given **radius**\n",
    "\n",
    "In scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7728fdafa163e068668cea92cf8d79306b41d458",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import  RadiusNeighborsClassifier\n",
    "Model=RadiusNeighborsClassifier(radius=8.0)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "#summary of the predictions made by the classifier\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "#Accouracy score\n",
    "print('accuracy is ', accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e55a785373bf654e0d4b2a78693fab1c8a625acb"
   },
   "source": [
    "<a id=\"35\"></a> <br>\n",
    "## 7-3 Logistic Regression\n",
    "Logistic regression is the appropriate regression analysis to conduct when the dependent variable is **dichotomous** (binary). Like all regression analyses, the logistic regression is a **predictive analysis**.\n",
    "\n",
    "In statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail, win/lose, alive/dead or healthy/sick; these are represented by an indicator variable, where the two values are labeled \"0\" and \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55eb348cf69272192274cd0728a123796b459b55",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "Model = LogisticRegression()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0a1c2ccaa4f6e9c5e2e42c47a295ceef7abd3b9"
   },
   "source": [
    "<a id=\"36\"></a> <br>\n",
    "##  7-4 Passive Aggressive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d509b2111a143660dd5cb1f02ea2779e38295b77",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "Model = PassiveAggressiveClassifier()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52938b49082dac7b35dc627828838bf12924cc7f"
   },
   "source": [
    "<a id=\"37\"></a> <br>\n",
    "## 7-5 Naive Bayes\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"**probabilistic classifiers**\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "522d4a3fa874950d0850a5a9a4178ec763781ec3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "Model = GaussianNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "272c1db2587015ac12e9919d51eaa4f4b73cb408"
   },
   "source": [
    "<a id=\"38\"></a> <br>\n",
    "##  7-6 MultinomialNB\n",
    "The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c370fd6ae6eee484a22cbf6c5323710222b05ffb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "Model = MultinomialNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e530d18ab308e36d575806583e534cc07fe61c61"
   },
   "source": [
    "<a id=\"39\"></a> <br>\n",
    "##  7-7 BernoulliNB\n",
    "Like MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e7051b5e9aa144b74e9913cb2a6668832e7f3e02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BernoulliNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "Model = BernoulliNB()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "386d2d0e4fc7f5dc2b9298226d8e2ecfb7150346"
   },
   "source": [
    "<a id=\"40\"></a> <br>\n",
    "## 7-8 SVM\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "* Effective in high dimensional spaces.\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples. \n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a44a5a43945404c95863668c2ba099f6032357f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "Model = SVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b1d092cc03dcaa712f4fe4ec6867b292321377d5"
   },
   "source": [
    "<a id=\"41\"></a> <br>\n",
    "## 7-9 Nu-Support Vector Classification\n",
    "\n",
    "> Similar to SVC but uses a parameter to control the number of support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2fa7c9a5bef780adb400bd9ad83d030f83a8d2b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine's \n",
    "from sklearn.svm import NuSVC\n",
    "\n",
    "Model = NuSVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d07a75e83251ddbf8cfdfd11c9faa2671ad87ff"
   },
   "source": [
    "<a id=\"42\"></a> <br>\n",
    "## 7-10 Linear Support Vector Classification\n",
    "\n",
    "Similar to **SVC** with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e7275f160f2e4e270200eaa01c13be5cb465142",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Support Vector Classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Model = LinearSVC()\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cec81c9e0c3bc6afba07811a321b5383a0f823f3"
   },
   "source": [
    "<a id=\"43\"></a> <br>\n",
    "## 7-11 Decision Tree\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for **classification** and **regression**. The goal is to create a model that predicts the value of a target variable by learning simple **decision rules** inferred from the data features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10e25ad67f7c25a8654637d4ba496b64121d67d0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decision Tree's\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Model = DecisionTreeClassifier()\n",
    "\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a7d897130fd705943764e924bbe468c99b7c036a"
   },
   "source": [
    "<a id=\"44\"></a> <br>\n",
    "## 7-12 ExtraTreeClassifier\n",
    "An extremely randomized tree classifier.\n",
    "\n",
    "Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the **max_features** randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n",
    "\n",
    "**Warning**: Extra-trees should only be used within ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a775006a814b6aacdcc07dc46995eb291b873f1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "Model = ExtraTreeClassifier()\n",
    "\n",
    "Model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = Model.predict(X_test)\n",
    "\n",
    "# Summary of the predictions made by the classifier\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# Accuracy score\n",
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48f940f73580a2997d75f22eba09d938c86a1a97"
   },
   "source": [
    "<a id=\"45\"></a> <br>\n",
    "## 7-13 Neural network\n",
    "\n",
    "I have used multi-layer Perceptron classifier.\n",
    "This model optimizes the log-loss function using **LBFGS** or **stochastic gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c10482510f654878f93b573dc1abe4112b861eb"
   },
   "source": [
    "## 7-13-1 What is a Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a94d82b497cbe543da0a637ecfed6e9e8b7569e7"
   },
   "source": [
    "There are many online examples and tutorials on perceptrons and learning. Here is a list of some articles:\n",
    "- [Wikipedia on Perceptrons](https://en.wikipedia.org/wiki/Perceptron)\n",
    "- Jurafsky and Martin (ed. 3), Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8e4da1a0b3d51a5fff38750fb4631ac3aa7eebb"
   },
   "source": [
    "This is an example that I have taken from a draft of the 3rd edition of Jurafsky and Martin, with slight modifications:\n",
    "We import *numpy* and use its *exp* function. We could use the same function from the *math* module, or some other module like *scipy*. The *sigmoid* function is defined as in the textbook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "196e5a54ed0de712e2254e77439051267cad4b3d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9aa207d0bb6a7342932406d8fa68cbd49be866b5"
   },
   "source": [
    "Our example data, **weights** $w$, **bias** $b$, and **input** $x$ are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69cae82df8a906ad43594464c6497e05e282dcd1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([0.2, 0.3, 0.8])\n",
    "b = 0.5\n",
    "x = np.array([0.5, 0.6, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cd5da02c0fc0beedd9a3417e399982293d28fd2"
   },
   "source": [
    "Our neural unit would compute $z$ as the **dot-product** $w \\cdot x$ and add the **bias** $b$ to it. The sigmoid function defined above will convert this $z$ value to the **activation value** $a$ of the unit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65d083572bf2cc897d816765db05758b107741ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = w.dot(x) + b\n",
    "print(\"z:\", z)\n",
    "print(\"a:\", sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e9101a20c9a167a1d925f3b64aafb94317155e2"
   },
   "source": [
    "### The XOR Problem\n",
    "The power of neural units comes from combining them into larger networks. Minsky and Papert (1969): A single neural unit cannot compute the simple logical function XOR.\n",
    "\n",
    "The task is to implement a simple **perceptron** to compute logical operations like AND, OR, and XOR.\n",
    "\n",
    "- Input: $x_1$ and $x_2$\n",
    "- Bias: $b = -1$ for AND; $b = 0$ for OR\n",
    "- Weights: $w = [1, 1]$\n",
    "\n",
    "with the following activation function:\n",
    "\n",
    "$$\n",
    "y = \\begin{cases}\n",
    "    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0\\\\\n",
    "    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0\n",
    "  \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c03b3a6a5307cc53e637afd6da5c307c985be7ac"
   },
   "source": [
    "We can define this activation function in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b0832df65fc1aa694a1cf67b8713c894b1ed2a2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27ae56a47bd180ebbfd82b3f0242c8078db6ac97"
   },
   "source": [
    "For AND we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3267d3f1a2864f29ff2382a2057b0b88fa74b649",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([1, 1])\n",
    "b = -1\n",
    "x = np.array([0, 0])\n",
    "print(\"0 AND 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 AND 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 AND 1:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 AND 1:\", activation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbee1f41f0bd66613f5b49e0f65be4bfd9f91283"
   },
   "source": [
    "For OR we could implement a perceptron as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf47440fb2f5a0fb016286f0a3a41a05a8416495",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.array([1, 1])\n",
    "b = 0\n",
    "x = np.array([0, 0])\n",
    "print(\"0 OR 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 0])\n",
    "print(\"1 OR 0:\", activation(w.dot(x) + b))\n",
    "x = np.array([0, 1])\n",
    "print(\"0 OR 1:\", activation(w.dot(x) + b))\n",
    "x = np.array([1, 1])\n",
    "print(\"1 OR 1:\", activation(w.dot(x) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "181068ef1b8e1ba568093184c41a118b4c0bfe7f"
   },
   "source": [
    "There is no way to implement a perceptron for XOR this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d12494da861ea094378c7cf6a3409803fb5585ac"
   },
   "source": [
    "no see our prediction for iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f040cfaeb71f8caa94e4d7f18cccde8d2a0b8a7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "Model=MLPClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "# Summary of the predictions\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffc339dbf9c8da74194b994930694bd97bb2afbb"
   },
   "source": [
    "<a id=\"46\"></a> <br>\n",
    "## 7-14 RandomForest\n",
    "A random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ed2305b51c2248a8aa62cf4452632f448e83771",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Model=RandomForestClassifier(max_depth=2)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1311eb15f2afceed2219faeb859d0d07b7072176"
   },
   "source": [
    "<a id=\"47\"></a> <br>\n",
    "## 7-15 Bagging classifier \n",
    "A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http://scikit-learn.org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c11c731d3db6c1c81301da85dc158cb7d324c4cb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "Model=BaggingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0944bd32424f38906148d96f4b1e6fccfbf97a6"
   },
   "source": [
    "<a id=\"48\"></a> <br>\n",
    "##  7-16 AdaBoost classifier\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "This class implements the algorithm known as **AdaBoost-SAMME** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "938946ee8e017b982c4c06e193d4d13cb7d3fb5f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Model=AdaBoostClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d62842d12731d3eb1d6577c5b35c12c4886c708"
   },
   "source": [
    "<a id=\"49\"></a> <br>\n",
    "## 7-17 Gradient Boosting Classifier\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "863124561c0d1b5995d0b8d3702daa7bc364d6b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "Model=GradientBoostingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e89b4494bd78c2d66beeba34a4e320fd8c9dae0c"
   },
   "source": [
    "<a id=\"50\"></a> <br>\n",
    "## 7-18 Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n",
    "\n",
    "These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0796cd9f1c902345df605b7557a9c3ff686e35a9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "Model=LinearDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "296137970fc94fa4a4eb4185cb5fa952b1985c57"
   },
   "source": [
    "<a id=\"51\"></a> <br>\n",
    "## 7-19 Quadratic Discriminant Analysis\n",
    "A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "\n",
    "The model fits a **Gaussian** density to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f521d19f295b8e8f24f5715e93b1c45e9a6bce3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "Model=QuadraticDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0518634bf8850ac1bfcfc301e93a8740e1995c3a"
   },
   "source": [
    "<a id=\"52\"></a> <br>\n",
    "## 7-20 Kmeans \n",
    "K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). \n",
    "\n",
    "The goal of this algorithm is **to find groups in the data**, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "427f08af02fc7288a5e35de5ff4b6c33b8fce491",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "iris_SP = dataset[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n",
    "# k-means cluster analysis for 1-15 clusters                                              \n",
    "from scipy.spatial.distance import cdist\n",
    "clusters=range(1,15)\n",
    "meandist=[]\n",
    "\n",
    "# loop through each cluster and fit the model to the train set\n",
    "# generate the predicted cluster assingment and append the mean \n",
    "# distance my taking the sum divided by the shape\n",
    "for k in clusters:\n",
    "    model=KMeans(n_clusters=k)\n",
    "    model.fit(iris_SP)\n",
    "    clusassign=model.predict(iris_SP)\n",
    "    meandist.append(sum(np.min(cdist(iris_SP, model.cluster_centers_, 'euclidean'), axis=1))\n",
    "    / iris_SP.shape[0])\n",
    "\n",
    "\"\"\"\n",
    "Plot average distance from observations from the cluster centroid\n",
    "to use the Elbow Method to identify number of clusters to choose\n",
    "\"\"\"\n",
    "plt.plot(clusters, meandist)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Average distance')\n",
    "plt.title('Selecting k with the Elbow Method') \n",
    "# pick the fewest number of clusters that reduces the average distance\n",
    "# If you observe after 3 we can see graph is almost linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adad2b1882db36d6ac1b4a47d2118386d6bdf0a1"
   },
   "source": [
    "<a id=\"53\"></a> <br>\n",
    "## 7-21-  Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0b9d339d6ccd1515d5c45f02358f351052b3c6f"
   },
   "source": [
    "Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.It is commonly used to train deep neural networks,a term referring to neural networks with more than one hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea58e2f8077aa74972727b0c3e10bc57fd82d5a7"
   },
   "source": [
    "In this example we will use a very simple network to start with. The network will only have one input and one output layer. We want to make the following predictions from the input:\n",
    "\n",
    "| Input  | Output |\n",
    "| ------ |:------:|\n",
    "| 0 0 1  | 0      |\n",
    "| 1 1 1  | 1      |\n",
    "| 1 0 1  | 1      |\n",
    "| 0 1 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07d2e4bc074dea99ec6c331a4ec51e777d468d0e"
   },
   "source": [
    "We will use *Numpy* to compute the network parameters, weights, activation, and outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b5c9c7115e49cdb5c3c8428399e2508687efa47"
   },
   "source": [
    "We will use the *[Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid)* activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8422345e9e9c1853acba702d0d7138e3a209f0e8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eec604fbe5729fecabfc4101a7b4887bf390876"
   },
   "source": [
    "We could use the [ReLU](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-relu) activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "557945abedb17c571926af334d5469b4edf12b48",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"The ReLU activation function.\"\"\"\n",
    "    return max(0, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f405e20e385bc594337a59deba1c7d8d7769349d"
   },
   "source": [
    "The [Sigmoid](http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid) activation function introduces non-linearity to the computation. It maps the input value to an output value between $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d388972329e79c533e7a8b90a2cfed18e288e4e5"
   },
   "source": [
    "<img src=\"http://s8.picofile.com/file/8339774900/SigmoidFunction1.png\" style=\"max-width:100%; width: 30%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6881ae9a1a8554739f9464119d14875beaaf8ad"
   },
   "source": [
    "The derivative of the sigmoid function is maximal at $x=0$ and minimal for lower or higher values of $x$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dfb7cad18d4509840dea33c4aff0e6de8a6cf86b"
   },
   "source": [
    "<img src=\"http://s9.picofile.com/file/8339770650/sigmoid_prime.png\" style=\"max-width:100%; width: 25%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "616e01ca7ed747aeb817707cbd14c57a92f31553"
   },
   "source": [
    "The *sigmoid_prime* function returns the derivative of the sigmoid for any given $z$. The derivative of the sigmoid is $z * (1 - z)$. This is basically the slope of the sigmoid function at any given point: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d4be4a6c8353a98c55eba479f520cbdeecc29f1d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"The derivative of sigmoid for z.\"\"\"\n",
    "    return z * (1 - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3be8c0c576f52ec92539039615a0b4ffe5248f57"
   },
   "source": [
    "We define the inputs as rows in *X*. There are three input nodes (three columns per vector in $X$. Each row is one trainig example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "14fa62c6d5ac721998095f5b556610e0984eea98",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([ [ 0, 0, 1 ],\n",
    "               [ 0, 1, 1 ],\n",
    "               [ 1, 0, 1 ],\n",
    "               [ 1, 1, 1 ] ])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3f32526b2e047abdda2a1d1d611f1cf7f188f2e9"
   },
   "source": [
    "The outputs are stored in *y*, where each row represents the output for the corresponding input vector (row) in *X*. The vector is initiated as a single row vector and with four columns and transposed (using the $.T$ method) into a column vector with four rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dbd824213ef3fe2dd7eb01d4b4396d816bf002d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0,0,1,1]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e6e2ad9b6539e5c3d6aff5cf2db5f44c0f2611e"
   },
   "source": [
    "To make the outputs deterministic, we seed the random number generator with a constant. This will guarantee that every time you run the code, you will get the same random distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a441227d4cd02b6654e7800cb823e8eef62ff1d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7cc15f07e9c4a15ec48265dd70f1e60e292f6485"
   },
   "source": [
    "We create a weight matrix ($Wo$) with randomly initialized weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1be1de240c6e7b718def61d73d5b7b4f770b54a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_outputs = 1\n",
    "#Wo = 2 * np.random.random( (n_inputs, n_outputs) ) - 1\n",
    "Wo = np.random.random( (n_inputs, n_outputs) ) * np.sqrt(2.0/n_inputs)\n",
    "print(Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f690ef1844d5344c2513381b6169f3bfe6080714"
   },
   "source": [
    "The reason for the output weight matrix ($Wo$) to have 3 rows and 1 column is that it represents the weights of the connections from the three input neurons to the single output neuron. The initialization of the weight matrix is random with a mean of $0$ and a variance of $1$. There is a good reason for chosing a mean of zero in the weight initialization. See for details the section on Weight Initialization in the [Stanford course CS231n on Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/neural-networks-2/#init)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b87807dc117e0908ec98b4e54843dc8f29c7397"
   },
   "source": [
    "The core representation of this network is basically the weight matrix *Wo*. The rest, input matrix, output vector and so on are components that we need to learning and evaluation. The leraning result is stored in the *Wo* weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb1319477d80a209e9caa3129b6371d872dab29b"
   },
   "source": [
    "We loop in the optimization and learning cycle 10,000 times. In the *forward propagation* line we process the entire input matrix for training. This is called **full batch** training. I do not use an alternative variable name to represent the input layer, instead I use the input matrix $X$ directly here. Think of this as the different inputs to the input neurons computed at once. In principle the input or training data could have many more training examples, the code would stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45907fc6c64b83116860d2c3dab8d252fd470c3e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(10000):\n",
    "    # forward propagation\n",
    "    l1 = sigmoid(np.dot(X, Wo))\n",
    "    \n",
    "    # compute the loss\n",
    "    l1_error = y - l1\n",
    "    #print(\"l1_error:\\n\", l1_error)\n",
    "    \n",
    "    # multiply the loss by the slope of the sigmoid at l1\n",
    "    l1_delta = l1_error * sigmoid_prime(l1)\n",
    "    #print(\"l1_delta:\\n\", l1_delta)\n",
    "    \n",
    "    #print(\"error:\", l1_error, \"\\nderivative:\", sigmoid(l1, True), \"\\ndelta:\", l1_delta, \"\\n\", \"-\"*10, \"\\n\")\n",
    "    # update weights\n",
    "    Wo += np.dot(X.T, l1_delta)\n",
    "\n",
    "print(\"l1:\\n\", l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d887640b00bf8d73c4544ef23d11c296ff15154"
   },
   "source": [
    "The dots in $l1$ represent the lines in the graphic below. The lines represent the slope of the sigmoid in the particular position. The slope is highest with a value $x = 0$ (blue dot). It is rather shallow with $x = 2$ (green dot), and not so shallow and not as high with $x = -1$. All derivatives are between $0$ and $1$, of course, that is, no slope or a maximal slope of $1$. There is no negative slope in a sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "01ea3085bf5e9dd4babee8437895735c9aa54763"
   },
   "source": [
    "<img src=\"http://s8.picofile.com/file/8339770734/sigmoid_deriv_2.png\" style=\"max-width:100%; width: 50%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0cfdf91801ec2bbffc34a7a95f6e899ed7a39fc"
   },
   "source": [
    "The matrix $l1\\_error$ is a 4 by 1 matrix (4 rows, 1 column). The derivative matrix $sigmoid\\_prime(l1)$ is also a 4 by one matrix. The returned matrix of the element-wise product $l1\\_delta$ is also the 4 by 1 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5349a8fcd505990baa71481697397cef4f2176fb"
   },
   "source": [
    "The product of the error and the slopes **reduces the error of high confidence predictions**. When the sigmoid slope is very shallow, the network had a very high or a very low value, that is, it was rather confident. If the network guessed something close to $x=0, y=0.5$, it was not very confident. Such predictions without confidence are updated most significantly. The other peripheral scores are multiplied with a number closer to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "057f51f99a2a87073013e9e2b9f5538c98297cf3"
   },
   "source": [
    "In the prediction line $l1 = sigmoid(np.dot(X, Wo))$ we compute the dot-product of the input vectors with the weights and compute the sigmoid on the sums.\n",
    "The result of the dot-product is the number of rows of the first matrix ($X$) and the number of columns of the second matrix ($Wo$).\n",
    "In the computation of the difference between the true (or gold) values in $y$ and the \"guessed\" values in $l1$ we have an estimate of the miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c64e2c97d7093bb8f71d5bd173820b526fc71a1"
   },
   "source": [
    "An example computation for the input $[ 1, 0, 1 ]$ and the weights $[ 9.5, 0.2, -0.1 ]$ and an output of $0.99$: If $y = 1$, the $l1\\_error = y - l2 = 0.01$, and $l1\\_delta = 0.01 * tiny\\_deriv$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "602abaf7339d617f2e0333c86ab98b94bb40a96d"
   },
   "source": [
    "<img src=\"http://s8.picofile.com/file/8339770792/toy_network_deriv.png\" style=\"max-width:100%; width: 40%; max-width: none\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3307f0633c665a5fa0aaa97898ab6a1d1d9f6fac"
   },
   "source": [
    "## 9-1 More Complex Example with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "750b541e55ae336dcee16ff38d672dff649792a6"
   },
   "source": [
    "Consider now a more complicated example where no column has a correlation with the output:\n",
    "\n",
    "| Input  | Output |\n",
    "| ------ |:------:|\n",
    "| 0 0 1  | 0      |\n",
    "| 0 1 1  | 1      |\n",
    "| 1 0 1  | 1      |\n",
    "| 1 1 1  | 0      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85bc7295eaf21d0bfad6c4572bfb868d74b2e1aa"
   },
   "source": [
    "The pattern here is our XOR pattern or problem: If there is a $1$ in either column $1$ or $2$, but not in both, the output is $1$ (XOR over column $1$ and $2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "340ea076b1899802093da816d92017ce2b3d2716"
   },
   "source": [
    "From our discussion of the XOR problem we remember that this is a *non-linear pattern*, a **one-to-one relationship between a combination of inputs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b64402f35850471c7136b201c783b9d9e60e6556"
   },
   "source": [
    "To cope with this problem, we need a network with another layer, that is a layer that will combine and transform the input, and an additional layer will map it to the output. We will add a *hidden layer* with randomized weights and then train those to optimize the output probabilities of the table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28829b11334a35dc1649f1d6d2134ed4e0d1a795"
   },
   "source": [
    "We will define a new $X$ input matrix that reflects the above table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c41f4546c675916ff0964370bc98a6862353666",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1],\n",
    "              [0, 1, 1],\n",
    "              [1, 0, 1],\n",
    "              [1, 1, 1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20aff6095cd099acf174500e76946c12481192d7"
   },
   "source": [
    "We also define a new output matrix $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "508e0c65bdd467bfed8d69ac52638d8821dd9e2f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[ 0, 1, 1, 0]]).T\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "87a03d01b3b1df0ff847a7507cc66ae87f1669d2"
   },
   "source": [
    "We initialize the random number generator with a constant again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "237da33769d159e1d450e66c93e677618c0a127f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "03c37d00f1d7b87c44e7a4777990049af4044548"
   },
   "source": [
    "Assume that our 3 inputs are mapped to 4 hidden layer ($Wh$) neurons, we have to initialize the hidden layer weights in a 3 by 4 matrix. The outout layer ($Wo$) is a single neuron that is connected to the hidden layer, thus the output layer is a 4 by 1 matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "823a9291e688f1840b781b5aeba9779215206676",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_hidden_neurons = 4\n",
    "n_output_neurons = 1\n",
    "Wh = np.random.random( (n_inputs, n_hidden_neurons) )  * np.sqrt(2.0/n_inputs)\n",
    "Wo = np.random.random( (n_hidden_neurons, n_output_neurons) )  * np.sqrt(2.0/n_hidden_neurons)\n",
    "print(\"Wh:\\n\", Wh)\n",
    "print(\"Wo:\\n\", Wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d27aca98d06bc0ed16678628d6927015b8dfe427"
   },
   "source": [
    "We will loop now 60,000 times to optimize the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90add134167965958223083f3c3a7818795777bc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    l1 = sigmoid(np.dot(X, Wh))\n",
    "    l2 = sigmoid(np.dot(l1, Wo))\n",
    "    \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (i % 10000) == 0:\n",
    "        print(\"Error:\", np.mean(np.abs(l2_error)))\n",
    "    \n",
    "    # gradient, changing towards the target value\n",
    "    l2_delta = l2_error * sigmoid_prime(l2)\n",
    "    \n",
    "    # compute the l1 contribution by value to the l2 error, given the output weights\n",
    "    l1_error = l2_delta.dot(Wo.T)\n",
    "    \n",
    "    # direction of the l1 target:\n",
    "    # in what direction is the target l1?\n",
    "    l1_delta = l1_error * sigmoid_prime(l1)\n",
    "    \n",
    "    Wo += np.dot(l1.T, l2_delta)\n",
    "    Wh += np.dot(X.T, l1_delta)\n",
    "\n",
    "print(\"Wo:\\n\", Wo)\n",
    "print(\"Wh:\\n\", Wh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "263abd8f132d7b73ab2eb67c56caf1508a35d1d3"
   },
   "source": [
    "The new computation in this new loop is $l1\\_error = l2\\_delta.dot(Wo.T)$, a **confidence weighted error** from $l2$ to compute an error for $l1$. The computation sends the error across the weights from $l2$ to $l1$. The result is a **contribution weighted error**, because we learn how much each node value in $l1$ **contributed** to the error in $l2$. This step is called **backpropagation**. We update $Wh$ using the same steps we did in the 2 layer implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97adc471c068fbd8d36ca19a4db0d98b0924c731"
   },
   "source": [
    "-----------------\n",
    "<a id=\"54\"></a> <br>\n",
    "# 10- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1adfb5ba84e0f1d8fba58a2fca30546ead095047",
    "collapsed": true
   },
   "source": [
    "In this kernel, I have tried to cover all the parts related to the process of **Machine Learning** with a variety of Python packages and I know that there are still some problems then I hope to get your feedback to improve it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cf3679a51c72dbe2d2549b5fe97e4ac5f1fa0fa0"
   },
   "source": [
    "you can follow me on:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani)\n",
    "> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    " **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "<a id=\"55\"></a> <br>\n",
    "\n",
    "-----------\n",
    "\n",
    "# 11- References\n",
    "* [1] [Iris image](https://rpubs.com/wjholst/322258)\n",
    "* [2] [IRIS](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "* [3] [https://skymind.ai/wiki/machine-learning-workflow](https://skymind.ai/wiki/machine-learning-workflow)\n",
    "* [4] [IRIS-wiki](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "* [5] [Problem-define](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
    "* [6] [Sklearn](http://scikit-learn.org/)\n",
    "* [7] [machine-learning-in-python-step-by-step](https://machinelearningmastery.com/machine-learning-in-python-step-by-step/)\n",
    "* [8] [Data Cleaning](http://wp.sigmod.org/?p=2288)\n",
    "* [9] [competitive data science](https://www.coursera.org/learn/competitive-data-science/)\n",
    "\n",
    "\n",
    "-------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
