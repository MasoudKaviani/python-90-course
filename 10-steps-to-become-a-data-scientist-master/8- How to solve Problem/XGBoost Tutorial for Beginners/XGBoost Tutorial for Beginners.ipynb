{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "726661972b09b03a31d424ef02a9be0cd284d81b"
   },
   "source": [
    " # <div style=\"text-align: center\">XGBoost Tutorial for Beginners \n",
    "<div style=\"text-align: center\">One of the most common questions we get on <b>Data science</b> is:\n",
    "<br>\n",
    "How can we provide better solutions than  <b>other machine learning algorithms?</b>\n",
    "<br>\n",
    "If you get confused and ask experts what should you learn at this stage, most of them would suggest / agree that you go ahead with ensemble learning? \n",
    "<br>\n",
    "In this simple tutorials you can learn all of the thing you need for  <b>using XGBoost as a method</b></div>\n",
    "<img src='http://s8.picofile.com/file/8341480568/XGBoost.png'>\n",
    "<div style=\"text-align:center\">last update: <b>11/01/2018</b></div>\n",
    "\n",
    "\n",
    "you can follow me on:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani)\n",
    "> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    " **I hope you find this kernel helpful and some <font color='red'> UPVOTES</font> would be very much appreciated**\n",
    " \n",
    " -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a01be35950f7a117fc6700e866de3bf5a3ea6b9"
   },
   "source": [
    "\n",
    "## Notebook  Content\n",
    "   [Introduction](#0)\n",
    "1. [ Why XGBoost?](#1)\n",
    "1. [Installing XGBoost ](#2)\n",
    "1. [Matrix Multiplication](#3)\n",
    "    1. [Vector-Vector Products](#4)\n",
    "    1. [Outer Product of Two Vectors](#5)\n",
    "    1. [Matrix-Vector Products](#6)\n",
    "    1. [Matrix-Matrix Products](#7)\n",
    "1. [Conclusion](#30)\n",
    "1. [References](#31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b18443661b6d30ffea2150fa74d44d62e14ae952"
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "#  1- Introduction\n",
    "* **XGBoost** is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data.\n",
    "* **XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance.\n",
    "* **XGBoost** is short for e**X**treme **G**radient **Boost**ing package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8a6fb87ba874c6108aa7266d80c20e161076c40b"
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## 2- Why XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "787e4b715d1969126ec6757bbb28f9c1fc84e970"
   },
   "source": [
    "* Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n",
    "\n",
    "* Core algorithm is parallelizable : Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPU’s and across networks of computers making it feasible to train on very large datasets as well.\n",
    "\n",
    "* Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n",
    "\n",
    "* Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n",
    "* Win competition On Kaggle : there are a lot of winners on Kaggle that use XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c212686b417d16cea9998ef4446bbd3817b16792"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 3- Installing XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9efb9808940ca6795af40c18c0e263bf58cfd166"
   },
   "source": [
    "There is a comprehensive installation guide on the [XGBoost documentation website](http://xgboost.readthedocs.io/en/latest/build.html).\n",
    "\n",
    "### 3-1 XGBoost in R\n",
    "If you are an R user, the best place to get started is the [CRAN page for the xgboost package](https://cran.r-project.org/web/packages/xgboost/index.html).\n",
    "\n",
    "### 3-2 XGBoost in Python\n",
    "Installation instructions are available on the Python section of the XGBoost installation guide.\n",
    "\n",
    "The official Python Package Introduction is the best place to start when working with XGBoost in Python.\n",
    "\n",
    "To get started quickly, you can type:\n",
    "\n",
    ">sudo pip install xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5eca2345978273dd67cfccc0edfc2bc6f7d467f"
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## 4- Problem Definition\n",
    "I think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( **Problem Formalization**)\n",
    "\n",
    "Problem Definition has four steps that have illustrated in the picture below:\n",
    "<img src=\"http://s8.picofile.com/file/8338227734/ProblemDefination.png\">\n",
    "<a id=\"4\"></a> <br>\n",
    "### 4-1 Problem Feature\n",
    "we will use the classic Iris data set. This dataset contains information about three different types of Iris flowers:\n",
    "\n",
    "* Iris Versicolor\n",
    "* Iris Virginica\n",
    "* Iris Setosa\n",
    "\n",
    "The data set contains measurements of four variables :\n",
    "\n",
    "* sepal length \n",
    "* sepal width\n",
    "* petal length \n",
    "* petal width\n",
    " \n",
    "The Iris data set has a number of interesting features:\n",
    "\n",
    "1. One of the classes (Iris Setosa) is linearly separable from the other two. However, the other two classes are not linearly separable.\n",
    "\n",
    "2. There is some overlap between the Versicolor and Virginica classes, so it is unlikely to achieve a perfect classification rate.\n",
    "\n",
    "3. There is some redundancy in the four input variables, so it is possible to achieve a good solution with only three of them, or even (with difficulty) from two, but the precise choice of best variables is not obvious.\n",
    "\n",
    "**Why am I  using iris dataset:**\n",
    "\n",
    "1- This is a good project because it is so well understood.\n",
    "\n",
    "2- Attributes are numeric so you have to figure out how to load and handle data.\n",
    "\n",
    "3- It is a classification problem, allowing you to practice with perhaps an easier type of supervised learning algorithm.\n",
    "\n",
    "4- It is a multi-class classification problem (multi-nominal) that may require some specialized handling.\n",
    "\n",
    "5- It only has 4 attributes and 150 rows, meaning it is small and easily fits into memory (and a screen or A4 page).\n",
    "\n",
    "6- All of the numeric attributes are in the same units and the same scale, not requiring any special scaling or transforms to get started.[5]\n",
    "\n",
    "7- we can define problem as clustering(unsupervised algorithm) project too.\n",
    "<a id=\"5\"></a> <br>\n",
    "### 4-2 Aim\n",
    "The aim is to classify iris flowers among three species (setosa, versicolor or virginica) from measurements of length and width of sepals and petals\n",
    "<a id=\"6\"></a> <br>\n",
    "### 4-3 Variables\n",
    "The variables are :\n",
    "**sepal_length**: Sepal length, in centimeters, used as input.\n",
    "**sepal_width**: Sepal width, in centimeters, used as input.\n",
    "**petal_length**: Petal length, in centimeters, used as input.\n",
    "**petal_width**: Petal width, in centimeters, used as input.\n",
    "**setosa**: Iris setosa, true or false, used as target.\n",
    "**versicolour**: Iris versicolour, true or false, used as target.\n",
    "**virginica**: Iris virginica, true or false, used as target.\n",
    "\n",
    "**<< Note >>**\n",
    "> You must answer the following question:\n",
    "How does your company expact to use and benfit from your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dee35ee214871c7c9d22104aafc6587eb3d2aa1e"
   },
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## 5- Inputs & Outputs\n",
    "<a id=\"8\"></a> <br>\n",
    "### 5-1 Inputs\n",
    "**Iris** is a very popular **classification** and **clustering** problem in machine learning and it is such as \"Hello world\" program when you start learning a new programming language. then I decided to apply Iris on  20 machine learning method on it.\n",
    "The Iris flower data set or Fisher's Iris data set is a **multivariate data set** introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers in three related species. Two of the three species were collected in the Gaspé Peninsula \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus\".\n",
    "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n",
    "\n",
    "As a result, **iris dataset is used as the input of all algorithms**.\n",
    "<a id=\"9\"></a> <br>\n",
    "### 5-2 Outputs\n",
    "the outputs for our algorithms totally depend on the type of classification or clustering algorithms.\n",
    "the outputs can be the number of clusters or predict for new input.\n",
    "\n",
    "**setosa**: Iris setosa, true or false, used as target.\n",
    "**versicolour**: Iris versicolour, true or false, used as target.\n",
    "**virginica**: Iris virginica, true or false, used as a target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18e6a0730989363caa069a745b5f3ea8b30766e9"
   },
   "source": [
    "###### <a id=\"4\"></a> <br>\n",
    "## 6- Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "5b8aa15d1b11789c38f1dd19d5f06e4be054e525"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import get_dummies\n",
    "import plotly.graph_objs as go\n",
    "from sklearn import datasets\n",
    "import plotly.plotly as py\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import warnings\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "49d5cacd5d0aeadd10836b930cdb43e0ed581a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib: 2.2.3\n",
      "sklearn: 0.20.0\n",
      "scipy: 1.1.0\n",
      "seaborn: 0.8.1\n",
      "pandas: 0.23.4\n",
      "numpy: 1.15.3\n",
      "Python: 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "print('scipy: {}'.format(scipy.__version__))\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('Python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5c5a1da5ce973e4dce69388b76022b5f69e4c16"
   },
   "source": [
    "<a id=\"17\"></a> <br>\n",
    "## 6-1 Data Collection\n",
    "**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n",
    "\n",
    "**Iris dataset**  consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1f4c3ec8ecd51cc0ae810666af8f93d6d1d27aaf"
   },
   "outputs": [],
   "source": [
    "# import Dataset to play with it\n",
    "dataset = pd.read_csv('../input/Iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6db3370c16c7e91d1d1624bc88a35cde1f8f141"
   },
   "source": [
    "**<< Note 1 >>**\n",
    "\n",
    "* Each row is an observation (also known as : sample, example, instance, record)\n",
    "* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "72cc7c7b60a33390a85b16bc34e3b9e424650cdd"
   },
   "source": [
    "<a id=\"32\"></a> <br>\n",
    "## 7- Model Deployment\n",
    "In this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of ML technique.\n",
    "\n",
    "> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b7788bbaaace438242d3b2d0d2ed489a91939ce"
   },
   "source": [
    "## 7-1 Families of ML algorithms\n",
    "There are several categories for machine learning algorithms, below are some of these categories:\n",
    "* Linear\n",
    "    * Linear Regression\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines\n",
    "* Tree-Based\n",
    "    * Decision Tree\n",
    "    * Random Forest\n",
    "    * GBDT\n",
    "* KNN\n",
    "* Neural Networks\n",
    "\n",
    "-----------------------------\n",
    "And if we  want to categorize ML algorithms with the type of learning, there are below type:\n",
    "* Classification\n",
    "\n",
    "    * k-Nearest \tNeighbors\n",
    "    * LinearRegression\n",
    "    * SVM\n",
    "    * DT \n",
    "    * NN\n",
    "    \n",
    "* clustering\n",
    "\n",
    "    * K-means\n",
    "    * HCA\n",
    "    * Expectation Maximization\n",
    "    \n",
    "* Visualization \tand\tdimensionality \treduction:\n",
    "\n",
    "    * Principal \tComponent \tAnalysis(PCA)\n",
    "    * Kernel PCA\n",
    "    * Locally -Linear\tEmbedding \t(LLE)\n",
    "    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n",
    "    \n",
    "* Association \trule\tlearning\n",
    "\n",
    "    * Apriori\n",
    "    * Eclat\n",
    "* Semisupervised learning\n",
    "* Reinforcement Learning\n",
    "    * Q-learning\n",
    "* Batch learning & Online learning\n",
    "* Ensemble  Learning\n",
    "\n",
    "**<< Note >>**\n",
    "> Here is no method which outperforms all others for all tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "daf9910caba26e071ff560dbdaca079ee148e140"
   },
   "source": [
    "<a id=\"33\"></a> <br>\n",
    "## 7-2 Prepare Features & Targets\n",
    "First of all seperating the data into dependent(Feature) and independent(Target) variables.\n",
    "\n",
    "**<< Note 4 >>**\n",
    "* X==>>Feature\n",
    "* y==>>Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b06cb1191a0f52a904c52a918d1f999536e79bda"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be230bb37d65624a2be449771bd222620a54f99e"
   },
   "source": [
    "After loading the data via **pandas**, we should checkout what the content is, description and via the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffc339dbf9c8da74194b994930694bd97bb2afbb"
   },
   "source": [
    "<a id=\"46\"></a> <br>\n",
    "## 7-3 RandomForest\n",
    "A random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n",
    "\n",
    "The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ed2305b51c2248a8aa62cf4452632f448e83771"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Model=RandomForestClassifier(max_depth=2)\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1311eb15f2afceed2219faeb859d0d07b7072176"
   },
   "source": [
    "<a id=\"47\"></a> <br>\n",
    "## 7-4 Bagging classifier \n",
    "A Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http://scikit-learn.org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c11c731d3db6c1c81301da85dc158cb7d324c4cb"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "Model=BaggingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0944bd32424f38906148d96f4b1e6fccfbf97a6"
   },
   "source": [
    "<a id=\"48\"></a> <br>\n",
    "##  7-5 AdaBoost classifier\n",
    "\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "This class implements the algorithm known as **AdaBoost-SAMME** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "938946ee8e017b982c4c06e193d4d13cb7d3fb5f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "Model=AdaBoostClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9d62842d12731d3eb1d6577c5b35c12c4886c708"
   },
   "source": [
    "<a id=\"49\"></a> <br>\n",
    "## 7-6 Gradient Boosting Classifier\n",
    "GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "863124561c0d1b5995d0b8d3702daa7bc364d6b0"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "Model=GradientBoostingClassifier()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e89b4494bd78c2d66beeba34a4e320fd8c9dae0c"
   },
   "source": [
    "<a id=\"50\"></a> <br>\n",
    "## 7-7 Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n",
    "\n",
    "These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0796cd9f1c902345df605b7557a9c3ff686e35a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "Model=LinearDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "296137970fc94fa4a4eb4185cb5fa952b1985c57"
   },
   "source": [
    "<a id=\"51\"></a> <br>\n",
    "## 7-8 Quadratic Discriminant Analysis\n",
    "A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.\n",
    "\n",
    "The model fits a **Gaussian** density to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f521d19f295b8e8f24f5715e93b1c45e9a6bce3"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "Model=QuadraticDiscriminantAnalysis()\n",
    "Model.fit(X_train,y_train)\n",
    "y_pred=Model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_pred,y_test))\n",
    "#Accuracy Score\n",
    "print('accuracy is ',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "c2844da02baa83d6341747942805cfde56b2805f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "afc2a360fedd783e5e9d7bbc975c9c6f06a2ee72"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "# 9-Conclusion\n",
    "* That XGBoost is a library for developing fast and high performance gradient boosting tree models.\n",
    "* That XGBoost is achieving the best performance on a range of difficult machine learning tasks.\n",
    "* That you can use this library from the command line, Python and R and how to get started.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b132163ee07917a0ab100b93f6ed5545ce0de45d"
   },
   "source": [
    "you can follow me on:\n",
    "> ###### [ GitHub](https://github.com/mjbahmani)\n",
    "> ###### [Kaggle](https://www.kaggle.com/mjbahmani/)\n",
    "\n",
    " **I hope you find this kernel helpful and some upvotes would be very much appreciated**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5719a5ba111b65b20b53d538281ac773eb14471a"
   },
   "source": [
    "<a id=\"31\"></a> <br>\n",
    "# 10-References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aab5b3d8cb417250dc6baa081a579106900effba"
   },
   "source": [
    "* [1] [datacamp](https://www.datacamp.com/community/tutorials/xgboost-in-python)\n",
    "* [2] [Xgboost presentation](https://www.oreilly.com/library/view/data-science-from/9781491901410/ch04.html)\n",
    "* [3] [machinelearningmastery](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/)\n",
    "* [4] [analyticsvidhya](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "* [5] [Github](https://github.com/mjbahmani)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
